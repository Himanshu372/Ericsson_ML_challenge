{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ericsson_ML.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM4Ru00TxGjM",
        "colab_type": "code",
        "outputId": "6614f8ef-32e4-415e-e1d4-dea70681c6b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy-xFzGZdmRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fdf389a3-600f-43c4-f1ab-5ca388bd6407"
      },
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tqdm import tqdm\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
        "# from joblib import dump, load\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# from google.colab import drive\n",
        "# from xgboost import XGBClassifier\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, classification_report, mean_squared_error\n",
        "# from sklearn.svm import SVC\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# # from copy import copy\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# # from catboost import CatBoostClassifier, Pool, cv\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from tensorflow.keras import Sequential\n",
        "# from tensorflow.keras.layers import Dense\n",
        "# from sklearn.feature_selection import RFECV\n",
        "import lightgbm as lgb\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words('english'))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4901oupidxKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_train(filepath):\n",
        "    '''\n",
        "    Reads file from filepath, returns a dataframe with required columns\n",
        "    :param filepath: string path\n",
        "    :param column_list: list of Dataframe column\n",
        "    :return: Dataframe\n",
        "    '''\n",
        "    return pd.read_csv(filepath)\n",
        "\n",
        "#Splitting train and test\n",
        "def normalize_dataset(features_df):\n",
        "    \"\"\"\n",
        "    Normalize the dataset\n",
        "    :param features_dataframe:\n",
        "    :return: normalized_features_array\n",
        "    \"\"\" \n",
        "    #Using minmaxscaler from sklearn to normalize dataset\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    return min_max_scaler.fit_transform(features_df)\n",
        "\n",
        "#Splitting train and test\n",
        "def split_dataset(features_np_array, target_np_array, test_percentage, random_state):\n",
        "    \"\"\"\n",
        "    Split the dataset with train_percentage\n",
        "    :param dataset:\n",
        "    :param test_percentage:\n",
        "    :param feature_array:\n",
        "    :param target_array:\n",
        "    :return: train_x, test_x, train_y, test_y\n",
        "    \"\"\" \n",
        "    #Split dataset into train and test dataset\n",
        "    train_x, test_x, train_y, test_y = train_test_split(features_np_array, target_np_array, test_size = test_percentage, random_state = random_state)\n",
        "    return train_x, test_x, train_y, test_y\n",
        "\n",
        "def generate_csv(df,csv_name):\n",
        "    '''\n",
        "    Converting pred_labels to dataframe\n",
        "    :param csv_name:\n",
        "    :return: submission_csv\n",
        "    '''\n",
        "    return df.to_csv(csv_name, index = False)\n",
        "\n",
        "def preprocess_title(s):\n",
        "    '''\n",
        "    '''\n",
        "    if re.search(r'(.*)\\/', s) is not None:\n",
        "        k = re.search(r'(.*)\\/', s).groups()[0].lower() \n",
        "        k = k.replace('[sound recording]', '').strip()\n",
        "    elif re.findall(r'\\w+', s) is not None:\n",
        "        k = ' '.join(re.findall(r'\\w+', s)).lower()\n",
        "        k = k.replace('[sound recording]', '').strip()\n",
        "    return k\n",
        "\n",
        "def match_keywords(s):\n",
        "    '''\n",
        "    '''\n",
        "    m = ['music', 'sound recording', 'musical']\n",
        "    k = 0 \n",
        "    for each in m:\n",
        "        if each in s:\n",
        "            k = 1\n",
        "    else:\n",
        "        k = 0\n",
        "    return k\n",
        "\n",
        "def creator_string(s):\n",
        "    '''\n",
        "    '''\n",
        "    if s != 'None':\n",
        "        s = ''.join(s.split(',')[0:2]).lower()\n",
        "    return s\n",
        "\n",
        "\n",
        "\n",
        "def creator_birth_year(s):\n",
        "    '''\n",
        "    '''\n",
        "    k = re.findall(r'\\d{4}', s)\n",
        "    if len(k) != 0:\n",
        "        try:\n",
        "            k = sorted(k)[0]\n",
        "            k = (pd.to_datetime('now').year - pd.to_datetime(k, format = '%Y').year)\n",
        "        except:\n",
        "            k = pd.period_range(start = k, end = '1678', freq = 'Y')\n",
        "            k = len(k)\n",
        "            k = (pd.to_datetime('now').year - pd.to_datetime('1678', format = '%Y').year + k)\n",
        "    else:\n",
        "        k = 0\n",
        "    return k\n",
        "    \n",
        "\n",
        "def subject_tokens(s):\n",
        "    '''\n",
        "    '''\n",
        "    if s != 'None':\n",
        "        s = s.split(',')\n",
        "        k = [re.sub(r'\\d+', '', i).strip().lower() for i in s]\n",
        "    else:\n",
        "        k = []\n",
        "    return k\n",
        "\n",
        "def subject_min_year_token(s):\n",
        "    '''\n",
        "    '''\n",
        "    k = re.findall(r'\\d{4}', s)\n",
        "    if len(k) != 0:\n",
        "        try:\n",
        "            k = sorted(k)[0]\n",
        "            k = (pd.to_datetime('now').year - pd.to_datetime(k, format = '%Y').year)\n",
        "        except:\n",
        "            k = pd.period_range(start = k, end = '1678', freq = 'Y')\n",
        "            k = len(k)\n",
        "            k = (pd.to_datetime('now').year - pd.to_datetime('1678', format = '%Y').year + k)\n",
        "    else:\n",
        "        k = 0\n",
        "    return k\n",
        "\n",
        "def get_publisher(s):\n",
        "    '''\n",
        "    '''\n",
        "    if re.search(r'(.*)\\/', s) is not None:\n",
        "        k = re.search(r'(.*)\\/', s).groups()[0].lower()\n",
        "    elif len(s.split(',')) != 0:\n",
        "        k = ''.join(s.split(',')[0:2]).lower()\n",
        "    return k\n",
        "\n",
        "def publication_year(s):\n",
        "    '''\n",
        "    '''\n",
        "    k = re.findall(r'\\d{4}', s)\n",
        "    if len(k) != 0:\n",
        "        try:\n",
        "            k = sorted(k)[0]\n",
        "            k = (pd.to_datetime('now').year - pd.to_datetime(k, format = '%Y').year)\n",
        "        except:\n",
        "            k = pd.period_range(start = k, end = '1678', freq = 'Y')\n",
        "            k = len(k)\n",
        "            k = (pd.to_datetime('now').year - pd.to_datetime('1678', format = '%Y').year + k)\n",
        "    else:\n",
        "        k = 0\n",
        "    return k\n",
        "\n",
        "\n",
        "\n",
        "def split_dataset(features_np_array, target_np_array, test_percentage, random_state):\n",
        "    \"\"\"\n",
        "    Split the dataset with train_percentage\n",
        "    :param dataset:\n",
        "    :param test_percentage:\n",
        "    :param feature_array:\n",
        "    :param target_array:\n",
        "    :return: train_x, test_x, train_y, test_y\n",
        "    \"\"\" \n",
        "    #Split dataset into train and test dataset\n",
        "    train_x, test_x, train_y, test_y = train_test_split(features_np_array, target_np_array, test_size = test_percentage, random_state = random_state)\n",
        "    return train_x, test_x, train_y, test_y\n",
        "    \n",
        "def read_glove(glove_path):\n",
        "    '''\n",
        "\n",
        "    :return:\n",
        "    '''\n",
        "    embeddings_index = dict()\n",
        "    f = open(glove_path, encoding='utf8')\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    return embeddings_index\n",
        "\n",
        "\n",
        "def normalize_dataset(features_df):\n",
        "    \"\"\"\n",
        "    Normalize the dataset\n",
        "    :param features_dataframe:\n",
        "    :return: normalized_features_array\n",
        "    \"\"\" \n",
        "    #Using minmaxscaler from sklearn to normalize dataset\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    return min_max_scaler.fit_transform(features_df)\n",
        "\n",
        "\n",
        "def prep_seq(col, word2vec_dict):\n",
        "    '''\n",
        "    '''\n",
        "    traintest_X = []\n",
        "    for sentence in tqdm(col.values):\n",
        "        sequence_words = np.zeros((word2vec_dict['cucumber'].shape))\n",
        "        for word in sentence:\n",
        "            if word in word2vec_dict.keys():\n",
        "                temp_X = word2vec_dict[word]\n",
        "            else:\n",
        "                temp_X = word2vec_dict['#']\n",
        "            sequence_words+=(temp_X)/len(sentence)\n",
        "        traintest_X.append(sequence_words)\n",
        "    return np.array(traintest_X)\n",
        "        \n",
        "\n",
        "    \n",
        "def label_to_mat(df_col):\n",
        "    '''\n",
        "\n",
        "    :param df_col:\n",
        "    :return:\n",
        "    '''\n",
        "    le = LabelEncoder()\n",
        "    le.fit(df_col.values)\n",
        "    labels = le.transform(df_col.values)\n",
        "    return to_categorical(labels, num_classes=len(le.classes_))\n",
        "\n",
        "\n",
        "def mat_to_label(df_col, np_array):\n",
        "    '''\n",
        "    \n",
        "    '''\n",
        "    le = LabelEncoder()\n",
        "    le.fit(df_col)\n",
        "    return le.inverse_transform(np_array)\n",
        "\n",
        "\n",
        "\n",
        "def generate_csv(df,csv_name):\n",
        "    '''\n",
        "    Converting pred_labels to dataframe\n",
        "    :param csv_name:\n",
        "    :return: submission_csv\n",
        "    '''\n",
        "    return df.to_csv(csv_name, index = False)\n",
        "        \n",
        "    \n",
        "def job_title_map(s):\n",
        "    '''\n",
        "    '''\n",
        "    if s != ' Anonymous Employee':\n",
        "        s = 'None'\n",
        "    return s\n",
        "   \n",
        "    \n",
        "def summary_tokens(s):\n",
        "    '''\n",
        "    '''\n",
        "    if s != 'None':\n",
        "        k = re.split(r'\\s|,', s)\n",
        "        k = [j for j in k if len(j) != 0]\n",
        "        k = [re.sub(r'\\d+', '', i).strip().lower() for i in k]\n",
        "        if len(k) > 5:\n",
        "            k = k[:5]\n",
        "        return k\n",
        "    \n",
        "def pos_neg_advice_tokens(s):\n",
        "    '''\n",
        "    '''\n",
        "    if s != 'None' and type(s) is not list:\n",
        "        k = re.split(r'\\s|,|\\/', s)\n",
        "        k = [re.sub(r'\\d+', '', i).strip().lower() for i in k]\n",
        "        k = [i.strip('+~*\\'\"?!,.():;').lower() for i in k]\n",
        "        k = sorted(set([j for j in k if len(j) != 0]), key = k.index)\n",
        "        k = [j for j in k if j not in stop_words]\n",
        "        if len(k) > 10:\n",
        "            k = k[:10]\n",
        "        return k\n",
        "    \n",
        "    \n",
        "    \n",
        "def merge_lists(l):\n",
        "    # merge lists and return unique words as string\n",
        "    tmp = []\n",
        "    for i in l:\n",
        "        if type(i) is list:\n",
        "            tmp = tmp + i\n",
        "    return tmp\n",
        "            \n",
        "         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBj8VeKfeDvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = process_train('/content/gdrive/My Drive/EricssonML/train_file.csv')\n",
        "test = process_train('/content/gdrive/My Drive/EricssonML/test_file.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAJVD972eM6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing \n",
        "\n",
        "# Replacing NAN with 'None' string\n",
        "train = train.replace(np.nan, 'None', regex = True)\n",
        "\n",
        "# Mapping UsageClass feature values\n",
        "train['UsageClass'] = train['UsageClass'].map({'Physical' : 0, 'Digital' : 1})\n",
        "\n",
        "# Extracting total_year, total_month and total_days difference from CheckoutYear\n",
        "\n",
        "train['CheckoutYear_year_diff'] = (pd.to_datetime('now') - pd.to_datetime(train['CheckoutYear'].astype(str) + '-0' + train['CheckoutMonth'].astype(str), format = '%Y-%M'))/np.timedelta64(1, 'Y')\n",
        "train['CheckoutYear_month_diff'] = (pd.to_datetime('now') - pd.to_datetime(train['CheckoutYear'].astype(str) + '-0' + train['CheckoutMonth'].astype(str), format = '%Y-%M'))/np.timedelta64(1, 'M')\n",
        "train['CheckoutYear_day_diff'] = (pd.to_datetime('now') - pd.to_datetime(train['CheckoutYear'].astype(str) + '-0' + train['CheckoutMonth'].astype(str), format = '%Y-%M'))/np.timedelta64(1, 'D')\n",
        "\n",
        "\n",
        "# Extracting title tokens\n",
        "train['Title_token'] = train['Title'].apply(lambda x : preprocess_title(x))\n",
        "\n",
        "#\n",
        "train['Creator_token'] = train['Creator'].apply(lambda x : creator_string(x))\n",
        "\n",
        "#\n",
        "train['Sound_recording_token'] = train['Title'].apply(lambda x : match_keywords(x))\n",
        "\n",
        "#\n",
        "train['Creator_birth_yr_diff'] = train['Creator'].apply(lambda x : creator_birth_year(x))\n",
        "\n",
        "#\n",
        "train['Subject_token'] = train['Subjects'].apply(lambda x : subject_tokens(x))\n",
        "\n",
        "#\n",
        "train['Publisher_token'] = train['Publisher'].apply(lambda x : get_publisher(x))\n",
        "\n",
        "#\n",
        "train['Publication_year_token'] = train['PublicationYear'].apply(lambda x : publication_year(x))\n",
        "\n",
        "# Appending tokens\n",
        "for i in range(len(train)):\n",
        "    for j in ['Title_token', 'Publisher_token', 'Creator_token']:\n",
        "        if train.loc[i, j].lower() != 'none':\n",
        "            train.loc[i, 'Subject_token'].append(train.loc[i, j])\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIWDeYCCXHXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing \n",
        "\n",
        "# Replacing NAN with 'None' string\n",
        "test = test.replace(np.nan, 'None', regex = True)\n",
        "\n",
        "# Mapping UsageClass feature values\n",
        "test['UsageClass'] = test['UsageClass'].map({'Physical' : 0, 'Digital' : 1})\n",
        "\n",
        "# Extracting total_year, total_month and total_days difference from CheckoutYear\n",
        "\n",
        "test['CheckoutYear_year_diff'] = (pd.to_datetime('now') - pd.to_datetime(test['CheckoutYear'].astype(str) + '-0' + test['CheckoutMonth'].astype(str), format = '%Y-%M'))/np.timedelta64(1, 'Y')\n",
        "test['CheckoutYear_month_diff'] = (pd.to_datetime('now') - pd.to_datetime(test['CheckoutYear'].astype(str) + '-0' + test['CheckoutMonth'].astype(str), format = '%Y-%M'))/np.timedelta64(1, 'M')\n",
        "test['CheckoutYear_day_diff'] = (pd.to_datetime('now') - pd.to_datetime(test['CheckoutYear'].astype(str) + '-0' + test['CheckoutMonth'].astype(str), format = '%Y-%M'))/np.timedelta64(1, 'D')\n",
        "\n",
        "\n",
        "# Extracting title tokens\n",
        "test['Title_token'] = test['Title'].apply(lambda x : preprocess_title(x))\n",
        "\n",
        "#\n",
        "test['Creator_token'] = test['Creator'].apply(lambda x : creator_string(x))\n",
        "\n",
        "#\n",
        "test['Sound_recording_token'] = test['Title'].apply(lambda x : match_keywords(x))\n",
        "\n",
        "#\n",
        "test['Creator_birth_yr_diff'] = test['Creator'].apply(lambda x : creator_birth_year(x))\n",
        "\n",
        "#\n",
        "test['Subject_token'] = test['Subjects'].apply(lambda x : subject_tokens(x))\n",
        "\n",
        "#\n",
        "test['Publisher_token'] = test['Publisher'].apply(lambda x : get_publisher(x))\n",
        "\n",
        "#\n",
        "test['Publication_year_token'] = test['PublicationYear'].apply(lambda x : publication_year(x))\n",
        "\n",
        "# Appending tokens\n",
        "for i in range(len(test)):\n",
        "    for j in ['Title_token', 'Publisher_token', 'Creator_token']:\n",
        "        if test.loc[i, j].lower() != 'none':\n",
        "            test.loc[i, 'Subject_token'].append(test.loc[i, j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1A_wNkMpqz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined_docs = train['Subject_token'].append(test['Subject_token'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08bJGzEZzYja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ca9bb3ee-74b8-4ac0-8012-e05364b5b16e"
      },
      "source": [
        "train['MaterialType'].value_counts()"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BOOK         21707\n",
              "SOUNDDISC     4149\n",
              "VIDEOCASS     2751\n",
              "VIDEODISC     1420\n",
              "SOUNDCASS     1020\n",
              "MIXED          347\n",
              "MUSIC          165\n",
              "CR              94\n",
              "Name: MaterialType, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l7L7grDh1Yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "feature_columns = ['UsageClass', 'Checkouts', 'CheckoutYear_year_diff', 'CheckoutYear_month_diff', 'CheckoutYear_day_diff', 'Sound_recording_token', 'Creator_birth_yr_diff', 'Publication_year_token']\n",
        "target_column = ['MaterialType']\n",
        "\n",
        "\n",
        "features = train[feature_columns]\n",
        "target = train[target_column]\n",
        "target = target['MaterialType'].map({y:x for x, y in enumerate(train['MaterialType'].unique())})\n",
        "\n",
        "\n",
        "\n",
        "train_features_array = normalize_dataset(features)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc1HgORkgvh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test_features_array = normalize_dataset(test[feature_columns])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO_Rz8keeTcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "399a9417-6269-47e7-cb83-5345bd695e5a"
      },
      "source": [
        "# Using GLOVE word embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip /content/glove.6B.zip\n",
        "embeddings_index = read_glove('/content/glove.6B.100d.txt')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-29 07:04:34--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-05-29 07:04:35--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-05-29 07:04:35--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  20.1MB/s    in 43s     \n",
            "\n",
            "2019-05-29 07:05:18 (19.3 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJkOkjU1rEBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4b3d3a5b-8a2c-40bc-838f-6763462bd3fe"
      },
      "source": [
        "# Preparing sequence for training\n",
        "\n",
        "train_w2v = prep_seq(train['Subject_token'], embeddings_index)\n",
        "test_w2v = prep_seq(test['Subject_token'], embeddings_index)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31653/31653 [00:00<00:00, 52949.99it/s]\n",
            "100%|██████████| 21102/21102 [00:00<00:00, 53510.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WS9udZRbMbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features_array = np.concatenate([train_features_array, train_w2v], axis=-1)\n",
        "test_features_array = np.concatenate([test_features_array, test_w2v], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWLPUC6jdxfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X, val_X, train_Y, val_Y = split_dataset(train_features_array, target, 0.3, 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dozuNth8hwXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using cross-val for predictions\n",
        "n_splits = 3\n",
        "splits = list(StratifiedKFold(n_splits = n_splits, shuffle = True ,random_state = 1111).split(train_X, train_Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0uwueBXYhpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modelling\n",
        "# Using LGBM\n",
        "lgb_param = {\n",
        "    'bagging_freq': 5,\n",
        "        'bagging_fraction': 0.35,\n",
        "        'boost_from_average':'false',\n",
        "        'boost': 'gbdt',\n",
        "        'feature_fraction': 0.0193,\n",
        "        'learning_rate': 0.0291,\n",
        "        'max_depth': -1,\n",
        "        'metric':'multi_logloss',\n",
        "        'min_data_in_leaf': 80,\n",
        "        'min_sum_hessian_in_leaf': 12,\n",
        "        'num_leaves': 8,    ### try 2; no interaction between variables makes sense here due to IID\n",
        "        'tree_learner': 'serial',\n",
        "        'objective': 'multiclass',\n",
        "        'num_class':8,\n",
        "        'verbosity': 1,\n",
        "        \"boost_from_average\": \"false\"\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zej40iQiFsNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "oof = np.zeros((len(train_Y), 8))\n",
        "predictions = np.zeros((len(test_features_array), 8))\n",
        "for i, (train_idx, valid_idx) in enumerate(splits):\n",
        "    \n",
        "    x = np.array(train_X)\n",
        "    y = np.array(train_Y)\n",
        "    \n",
        "    trn_data = lgb.Dataset(x[train_idx], label = y[train_idx])\n",
        "    val_data = lgb.Dataset(x[valid_idx], label = y[valid_idx])\n",
        "    \n",
        "    model = lgb.train(lgb_param, trn_data, 10000, valid_sets = [trn_data, val_data], early_stopping_rounds=10000, verbose_eval=1000)\n",
        "    \n",
        "    oof[valid_idx] = model.predict(x[valid_idx], num_iteration = model.best_iteration)\n",
        "        \n",
        "#     fold_importance_df = pd.DataFrame()\n",
        "#     fold_importance_df[\"feature\"] = features\n",
        "#     fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n",
        "#     fold_importance_df[\"fold\"] = i+1\n",
        "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "    \n",
        "    predictions += model.predict(test_features_array, num_iteration = model.best_iteration) / n_splits\n",
        "\n",
        "print(\"Classification report\".format(classification_report(train_Y, oof)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8C7-e1DlR83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = LabelEncoder()\n",
        "target = le.fit_transform(target)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2v5ui2OlYlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = le.inverse_transform(np.argmax(predictions, axis = 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T42GKLFEwM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_lgbm_pred_test = pd.DataFrame(predictions)\n",
        "test_uid_df = pd.DataFrame(test['ID'], columns = ['ID'])\n",
        "cv_lgbm_pred_test = test_uid_df.join(cv_lgbm_pred_test, how = 'inner')\n",
        "cv_lgbm_pred_test.columns = ['ID', 'MaterialType']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyjh6hPQEydA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "870227cc-c4ec-4a3d-bb01-83a7bbc8a4af"
      },
      "source": [
        "cv_lgbm_pred_test['MaterialType'].value_counts()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    20361\n",
              "1      634\n",
              "3       99\n",
              "2        8\n",
              "Name: MaterialType, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HT4sT6KJx-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_lgbm_pred_test['MaterialType'] = cv_lgbm_pred_test['MaterialType'].map({x:y for x, y in enumerate(train['MaterialType'].unique())})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRJzKronJ4RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_lgbm_pred_test['MaterialType'].value_counts()\n",
        "generate_csv(cv_lgbm_pred_test, 'cv_lgbm_pred_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A23c00IiKOX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rating problem\n",
        "\n",
        "train_rat = process_train('/content/gdrive/My Drive/EricssonML/Ratings/train.csv')\n",
        "test_rat = process_train('/content/gdrive/My Drive/EricssonML/Ratings/test.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUMUCqnBJj0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gJ9yCKsWfpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "# Mapping Place feature values\n",
        "train_rat['Place'] = train_rat['Place'].map({y:x for x, y in enumerate(train_rat['Place'].unique())})\n",
        "\n",
        "# Extracting month, year and days from review date\n",
        "\n",
        "train_rat['Date_year_diff'] = (pd.to_datetime('now') - pd.to_datetime(train_rat['date'].apply(lambda x : x.strip()).astype(str), errors = 'coerce', format = '%b %d, %Y'))/np.timedelta64(1, 'Y')\n",
        "\n",
        "train_rat['Date_month_diff'] = (pd.to_datetime('now') - pd.to_datetime(train_rat['date'].apply(lambda x : x.strip()).astype(str), errors = 'coerce', format = '%b %d, %Y'))/np.timedelta64(1, 'M')\n",
        "\n",
        "train_rat['Date_day_diff'] = (pd.to_datetime('now') - pd.to_datetime(train_rat['date'].apply(lambda x : x.strip()).astype(str), errors = 'coerce', format = '%b %d, %Y'))/np.timedelta64(1, 'D')\n",
        "\n",
        "train_rat['Date_year_diff'] = train_rat['Date_year_diff'].replace(np.nan, 0, regex = True)\n",
        "\n",
        "train_rat['Date_month_diff'] = train_rat['Date_month_diff'].replace(np.nan, 0, regex = True)\n",
        "\n",
        "train_rat['Date_day_diff'] = train_rat['Date_day_diff'].replace(np.nan, 0, regex = True)\n",
        "\n",
        "# Mapping status feature values\n",
        "train_rat['status'] = train_rat['status'].map({y:x for x, y in enumerate(train_rat['status'].unique())})\n",
        "\n",
        "# Mapping values for job_title\n",
        "train_rat['job_title'] = train_rat['job_title'].apply(lambda x: job_title_map(x)).map({' Anonymous Employee' : 0, 'None' : 1})\n",
        "\n",
        "#\n",
        "train_rat['summary'] = train_rat['summary'].replace(np.nan, 'None', regex = True)\n",
        "train_rat['summary_tokens'] = train_rat['summary'].apply(lambda x : pos_neg_advice_tokens(x))\n",
        " \n",
        "#\n",
        "train_rat['positives'] = train_rat['positives'].replace(np.nan, 'None', regex = True)\n",
        "train_rat['pos_tokens'] = train_rat['positives'].apply(lambda x : pos_neg_advice_tokens(x))\n",
        "\n",
        "#\n",
        "train_rat['negatives'] = train_rat['negatives'].replace(np.nan, 'None', regex = True)\n",
        "train_rat['neg_tokens'] = train_rat['negatives'].apply(lambda x : pos_neg_advice_tokens(x))\n",
        "\n",
        "#\n",
        "train_rat['advice_to_mgmt'] = train_rat['advice_to_mgmt'].replace(np.nan, 'None', regex = True)\n",
        "train_rat['advice_tokens'] = train_rat['advice_to_mgmt'].apply(lambda x : pos_neg_advice_tokens(x))\n",
        "train_rat['advice_tokens'].loc[train_rat['advice_tokens'].isnull()] = train_rat['advice_tokens'].loc[train_rat['advice_tokens'].isnull()].apply(lambda x : [])\n",
        "\n",
        "\n",
        "#\n",
        "train_rat['tokens'] = train_rat['pos_tokens'] + train_rat['neg_tokens'] + train_rat['advice_tokens']\n",
        "\n",
        "#\n",
        "train_rat['overall'] = train_rat['overall'].astype('object')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr934S1C1qCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "# Mapping Place feature values\n",
        "test_rat['Place'] = test_rat['Place'].map({y:x for x, y in enumerate(test_rat['Place'].unique())})\n",
        "\n",
        "# Extracting month, year and days from review date\n",
        "\n",
        "test_rat['Date_year_diff'] = (pd.to_datetime('now') - pd.to_datetime(test_rat['date'].apply(lambda x : x.strip()).astype(str), errors = 'coerce', format = '%b %d, %Y'))/np.timedelta64(1, 'Y')\n",
        "\n",
        "test_rat['Date_month_diff'] = (pd.to_datetime('now') - pd.to_datetime(test_rat['date'].apply(lambda x : x.strip()).astype(str), errors = 'coerce', format = '%b %d, %Y'))/np.timedelta64(1, 'M')\n",
        "\n",
        "test_rat['Date_day_diff'] = (pd.to_datetime('now') - pd.to_datetime(test_rat['date'].apply(lambda x : x.strip()).astype(str), errors = 'coerce', format = '%b %d, %Y'))/np.timedelta64(1, 'D')\n",
        "\n",
        "test_rat['Date_year_diff'] = test_rat['Date_year_diff'].replace(np.nan, 0, regex = True)\n",
        "\n",
        "test_rat['Date_month_diff'] = test_rat['Date_month_diff'].replace(np.nan, 0, regex = True)\n",
        "\n",
        "test_rat['Date_day_diff'] = test_rat['Date_day_diff'].replace(np.nan, 0, regex = True)\n",
        "\n",
        "# Mapping status feature values\n",
        "test_rat['status'] = test_rat['status'].map({y:x for x, y in enumerate(test_rat['status'].unique())})\n",
        "\n",
        "# Mapping values for job_title\n",
        "test_rat['job_title'] = test_rat['job_title'].apply(lambda x: job_title_map(x)).map({' Anonymous Employee' : 0, 'None' : 1})\n",
        "\n",
        "#\n",
        "test_rat['summary'] = test_rat['summary'].replace(np.nan, 'None', regex = True)\n",
        "test_rat['summary_tokens'] = test_rat['summary'].apply(lambda x : pos_neg_advice_tokens(x))\n",
        " \n",
        "#\n",
        "test_rat['positives'] = test_rat['positives'].replace(np.nan, 'None', regex = True)\n",
        "test_rat['pos_tokens'] = test_rat['positives'].apply(lambda x : pos_neg_advice_tokens(x))\n",
        "\n",
        "#\n",
        "test_rat['negatives'] = test_rat['negatives'].replace(np.nan, 'None', regex = True)\n",
        "test_rat['neg_tokens'] = test_rat['negatives'].apply(lambda x : pos_neg_advice_tokens(x))\n",
        "test_rat['neg_tokens'].loc[test_rat['neg_tokens'].isnull()] = test_rat['neg_tokens'].loc[test_rat['neg_tokens'].isnull()].apply(lambda x : [])\n",
        "\n",
        "\n",
        "#\n",
        "test_rat['advice_to_mgmt'] = test_rat['advice_to_mgmt'].replace(np.nan, 'None', regex = True)\n",
        "test_rat['advice_tokens'] = test_rat['advice_to_mgmt'].apply(lambda x : pos_neg_advice_tokens(x))\n",
        "test_rat['advice_tokens'].loc[test_rat['advice_tokens'].isnull()] = test_rat['advice_tokens'].loc[test_rat['advice_tokens'].isnull()].apply(lambda x : [])\n",
        "\n",
        "\n",
        "#\n",
        "test_rat['tokens'] = test_rat['pos_tokens'] + test_rat['neg_tokens'] + test_rat['advice_tokens']\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUxY7MVjXznw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "592e8cb0-1e56-4744-b1d5-fa8680e728fb"
      },
      "source": [
        "feature_columns = ['Place', 'Date_year_diff', 'Date_month_diff', 'Date_day_diff', 'status', 'job_title']\n",
        "target_column = ['overall']\n",
        "\n",
        "features = train_rat[feature_columns]\n",
        "target = train_rat[target_column]\n",
        "\n",
        "\n",
        "train_features_array = normalize_dataset(features)\n",
        "test_features_array = normalize_dataset(test_rat[feature_columns])\n",
        "\n",
        "le = LabelEncoder()\n",
        "target = le.fit_transform(target)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMdFB-3vX7PL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "069ed16b-b3db-4b14-cfdc-05abaedff6d5"
      },
      "source": [
        "train_w2v = prep_seq(train_rat['tokens'], embeddings_index)\n",
        "test_w2v = prep_seq(test_rat['tokens'], embeddings_index)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30336/30336 [00:02<00:00, 14691.84it/s]\n",
            "100%|██████████| 29272/29272 [00:02<00:00, 14635.48it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIxrAfN9aC-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features_array = np.concatenate([train_features_array, train_w2v], axis=-1)\n",
        "test_features_array = np.concatenate([test_features_array, test_w2v], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVm4NGDLAu_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X, val_X, train_Y, val_Y = split_dataset(train_features_array, target, 0.3, 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsmEtTNnaNYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH-_C1HU9A7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using cross-val for predictions\n",
        "n_splits = 3\n",
        "splits = list(StratifiedKFold(n_splits = n_splits, shuffle = True ,random_state = 1111).split(train_X, train_Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gxu-guY9Du6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modelling\n",
        "# Using LGBM\n",
        "lgb_param = {\n",
        "    'bagging_freq': 5,\n",
        "        'bagging_fraction': 0.35,\n",
        "        'boost_from_average':'false',\n",
        "        'boost': 'gbdt',\n",
        "        'feature_fraction': 0.0193,\n",
        "        'learning_rate': 0.0291,\n",
        "        'max_depth': -1,\n",
        "        'metric':'multi_error',\n",
        "        'min_data_in_leaf': 80,\n",
        "        'min_sum_hessian_in_leaf': 12,\n",
        "        'num_leaves': 8,    ### try 2; no interaction between variables makes sense here due to IID\n",
        "        'tree_learner': 'serial',\n",
        "        'objective': 'multiclass',\n",
        "        'num_class':5,\n",
        "        'verbosity': 1,\n",
        "        \"boost_from_average\": \"false\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdDn0f3E96rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "oof = np.zeros((len(train_Y), 5))\n",
        "predictions = np.zeros((len(test_features_array), 5))\n",
        "for i, (train_idx, valid_idx) in enumerate(splits):\n",
        "    \n",
        "    x = np.array(train_X)\n",
        "    y = np.array(train_Y)\n",
        "    \n",
        "    trn_data = lgb.Dataset(x[train_idx], label = y[train_idx])\n",
        "    val_data = lgb.Dataset(x[valid_idx], label = y[valid_idx])\n",
        "    \n",
        "    model = lgb.train(lgb_param, trn_data, 10000, valid_sets = [trn_data, val_data], early_stopping_rounds=10000, verbose_eval=1000)\n",
        "    \n",
        "    oof[valid_idx] = model.predict(x[valid_idx], num_iteration = model.best_iteration)\n",
        "        \n",
        "#     fold_importance_df = pd.DataFrame()\n",
        "#     fold_importance_df[\"feature\"] = features\n",
        "#     fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n",
        "#     fold_importance_df[\"fold\"] = i+1\n",
        "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "    \n",
        "    predictions += model.predict(test_features_array, num_iteration = model.best_iteration) / n_splits\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn7Icr-x99dS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = le.inverse_transform(np.argmax(predictions, axis = 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n0osftZ-kKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_lgbm_pred_test = pd.DataFrame(predictions)\n",
        "test_uid_df = pd.DataFrame(test_rat['ID'], columns = ['ID'])\n",
        "cv_lgbm_pred_test = test_uid_df.join(cv_lgbm_pred_test, how = 'inner')\n",
        "cv_lgbm_pred_test.columns = ['ID', 'overall']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cTrGzBcu-id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_lgbm_pred_test['overall'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvzNoLjG_en_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_csv(cv_lgbm_pred_test, 'cv_lgbm_pred_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuvxnCkk_kT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}